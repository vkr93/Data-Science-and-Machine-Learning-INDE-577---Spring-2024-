# The Multilayer Perceptron (Deep Neural Network)

## The Multilayer Perceptron

A multilayer feedforward network, also known as a multilayer perceptron (MLP), is a type of artificial neural network (ANN) that consists of multiple layers of nodes, each connected to the next. The nodes in each layer are fully connected to the nodes in the next layer, but there are no connections between nodes within the same layer. The following figures provides an examples of MLP architecture.

![Towards data science](https://miro.medium.com/v2/resize:fit:720/format:webp/1*MF1q2Q3fbpYlXX8fZUiwpA.png)

*   **Input Layer:** The input layer is where the MLP and dataset first engage with one another. A feature in the incoming data is matched to each neuron in this layer. For instance, each neuron might represent the intensity value of a pixel in picture categorization. These unprocessed input values are to be distributed to the neurons in the next hidden layers by the input layer.

*    **Hidden Layers:** MLPs have a hidden layer or layers that are present between the input and output layers. The main computations happen at these layers. Every neuron in a hidden layer analyzes the data that comes from the neurons in the layer above it. In the same buried layer, neurons do not interact directly with one another but rather indirectly via weighted connections. The hidden layer transformation allows the network to learn intricate links and representations in the data. The intricacy of the task might affect the depth (number of hidden layers) and width (number of neurons in each layer).

*   **Output Layer:** The MLP’s neurons in the output layer, the last layer, generate the model’s predictions. The structure of this layer is determined by the particular task at hand. The probability score for binary classification may be generated by a single neuron with a sigmoid activation function. Multiple neurons, often with softmax activation, can give probabilities to each class in a multi-class classification system. When doing regression tasks, the output layer frequently just has a single neuron that can forecast a continuous value.

*   **Connections and weights:** a network consistes of connects that transfer the output from neuron i to neuron j, assigned weight W_ij 
*   **Activation Function (aka transfer function):** defines output of that node given a set of inputs. Its behavior is similar to the behavior of a single perceptron, but it is the ability to use nonlinear activation functions that allow networks to computer nontrivial problems with a small number of nodes
*   **Learning rule:** Algorithm which modifies the parameters of the neural network, in order for a given input to the network to produce a favored output. This learning process is often just modifying the weights and thresholds

[image](https://towardsdatascience.com/multilayer-perceptron-explained-with-a-real-life-example-and-python-code-sentiment-analysis-cb408ee93141)

### Dataset

[IBMEmployeeAttrition](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset) dataset was used.


### Resources:
1.  [Neural Networks Supervised Learning](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)
2.  [Multilayer Perceptron](https://towardsdatascience.com/multilayer-perceptron-explained-with-a-real-life-example-and-python-code-sentiment-analysis-cb408ee93141)

